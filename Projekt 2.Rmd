---
title: "R projekt 2"
author: "Klaudia Rajca, Aleksandra Konopelska"
date: "2025-11-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

    
## 1. Wstęp i cel pracy

Skalowanie wielowymiarowe (MDS, *multidimensional scaling*) to metoda, która pozwala narysować na zwykłym wykresie obiekty opisane wieloma cechami liczbowymi. Każdy kraj w naszych danych jest opisany przez kilka zmiennych (np. różne wskaźniki społeczno-gospodarcze), więc można go traktować jako punkt w przestrzeni \(n\)-wymiarowej. MDS szuka dla tych punktów nowej reprezentacji w przestrzeni o mniejszym wymiarze \(m\) (najczęściej 2 lub 3), przy czym \(m < n\). Chodzi o to, żeby w nowej przestrzeni kraje, które były do siebie podobne, dalej leżały blisko siebie, a kraje bardzo różne, daleko od siebie.

W projekcie wykorzystujemy dwie techniki skalowania wielowymiarowego:

- **klasyczne skalowanie wielowymiarowe** (metryczne MDS, w R przy pomocy funkcji `cmdscale` z biblioteki `stats`),
- **skalowanie Sammona** (nieliniowe MDS, w R przy pomocy funkcji `sammon` z pakietu `MASS`).

Klasyczne MDS opiera się na odległościach euklidesowych i stara się jak najlepiej odwzorować rzeczywiste odległości między krajami w przestrzeni o niższym wymiarze. Metoda Sammona również korzysta z odległości, ale większy nacisk kładzie na dobre odwzorowanie małych odległości (bliższych sobie krajów) i jest metodą iteracyjną, nieliniową.


Drugą główną metodą w projekcie jest **analiza składowych głównych (PCA – Principal Component Analysis)**. PCA służy do zmniejszenia liczby zmiennych przy jak najmniejszej stracie informacji. Zamiast wielu oryginalnych zmiennych tworzymy kilka nowych zmiennych –> składowych głównych. Każda składowa główna jest liniową kombinacją zmiennych oryginalnych i opisuje określoną część całkowitej zmienności w danych.

W pracy zastosujemy dwa sposoby wykonania PCA w R:

- PCA obliczoną przy pomocy funkcji **`eigen`**, obliczającej wartości i wektory własne podanej macierzy,
- PCA obliczoną przy pomocy funkcji **`prcomp`** z biblioteki `stats`.

Wykorzystujemy dane z pliku *countries of the world.csv*, które opisują sytuację społeczno-gospodarczą państw świata. Traktujemy je jako dane przekrojowe, czyli pozwalające porównać kraje między sobą w jednym "momencie". 

Celem pracy jest:

- zastosowanie klasycznego MDS i metody Sammona do wizualizacji podobieństw i różnic między krajami,
- przeprowadzenie analizy składowych głównych (PCA) z użyciem funkcji `eigen` oraz `prcomp` i interpretacja uzyskanych składowych,
- porównanie wyników uzyskanych różnymi technikami MDS oraz porównanie ich z wynikami PCA, a także ocena, jak dobrze opisują one strukturę i zróżnicowanie krajów.


## 2. Wstępna Analiza (opis danych i ich przygotowanie)

### 2.1. Pakiety, których używamy

**Uwaga techniczna do kodu.**  
Niektóre pakiety w R mają funkcje o tych samych nazwach (np. `select()` w `dplyr` i w `MASS`). To może powodować błędy. Dlatego w kodzie piszemy, z którego pakietu bierzemy funkcję (`dplyr::select(...)`, `tidyr::pivot_longer(...)` albo `MASS::sammon(...)`). Dzięki temu R się nie myli i wszystko działa.


```{r message=FALSE, warning=FALSE}
library(visdat)
library(psych)
library(mice)
library(corrplot)
library(janitor)
library(knitr)
library(tidyverse)  # dplyr, tidyr, ggplot2 itd.
```


### 2.2. Opis zbioru „countries of the world”

Dane pochodzą z pliku “countries of the world.csv”, opracowanego na podstawie źródeł rządowych USA (US Government), obejmujących lata 1970–2017. Oznacza to, że dane nie dotyczą jednego konkretnego roku, lecz stanowią zestaw uśrednionych i orientacyjnych wskaźników opisujących sytuację społeczno-gospodarczą poszczególnych państw. W pracy traktujemy je jako **dane przekrojowe**, czyli takie, które pozwalają porównać kraje między sobą w jednym „momencie”, bez analizowania zmian w czasie.

Jedna obserwacja w zbiorze danych to **jeden kraj**. Dla każdego kraju dostępne są m.in. następujące zmienne liczbowe:

- `Population` – liczba ludności,
- `Area (sq. mi.)` – powierzchnia kraju w milach kwadratowych,
- `Pop. Density (per sq. mi.)` – gęstość zaludnienia,
- `Coastline (coast/area ratio)` – relacja długości linii brzegowej do powierzchni,
- `Net migration` – saldo migracji,
- `Infant mortality (per 1000 births)` – śmiertelność niemowląt na 1000 urodzeń,
- `GDP ($ per capita)` – produkt krajowy brutto na mieszkańca (w dolarach),
- `Literacy (%)` – odsetek ludności umiejącej czytać i pisać,
- `Phones (per 1000)` – liczba telefonów na 1000 mieszkańców,
- `Arable (%)`, `Crops (%)`, `Other (%)` – struktura użytkowania ziemi,
- `Climate` – wskaźnik klimatu (wartości liczbowe),
- `Birthrate` – współczynnik urodzeń,
- `Deathrate` – współczynnik zgonów,
- `Agriculture`, `Industry`, `Service` – udział rolnictwa, przemysłu i usług w gospodarce.

Dodatkowo mamy zmienne opisowe, takie jak `Country` (nazwa kraju) i `Region` (region/kontynent).

### 2.3. Wybór zmiennych do analizy

Do dalszej analizy wybieramy kilka zmiennych ilościowych, które w prosty sposób opisują poziom rozwoju społeczno-gospodarczego krajów. Będziemy korzystać z następujących zmiennych:

- `GDP ($ per capita)` – produkt krajowy brutto na mieszkańca (w dolarach),
- `Infant mortality (per 1000 births)` – śmiertelność niemowląt na 1000 urodzeń,
- `Literacy (%)` – odsetek ludności umiejącej czytać i pisać,
- `Phones (per 1000)` – liczba telefonów na 1000 mieszkańców,
- `Birthrate` – współczynnik urodzeń,
- `Deathrate` – współczynnik zgonów,
- `Agriculture`, `Industry`, `Service` – udział rolnictwa, przemysłu i usług w gospodarce.

Razem te zmienne obejmują kilka ważnych "wymiarów" rozwoju: gospodarkę, zdrowie, edukację, demografię i strukturę sektorową. Wszystkie są mierzone na skali metrycznej (liczbowe), więc nadają się zarówno do MDS (Wykorzystywane jest w przypadku zmiennych metrycznych), jak i do PCA. Dodatkowo dla tych zmiennych jest stosunkowo mało braków danych, więc nie tracimy zbyt wielu krajów przy czyszczeniu zbioru.

### 2.4. Wczytanie danych

Wczytujemy plik countries of the world.csv. Ustawiamy stringsAsFactors = FALSE, żeby zmienne tekstowe nie zamieniły się od razu w faktory (żeby napisy zostały zwykłymi stringami, u nas kraje).

```{r}
countries_raw <- read.csv("countries of the world.csv",header = TRUE,stringsAsFactors = FALSE)
```

Podgląd pierwszych wierszy
```{r}
head(countries_raw, 10)
```

Sprawdzenie struktury pliku
```{r}
str(countries_raw)
```

### 2.5. Wybór zmiennych i przygotowanie danych

Wybieramy zmienne ilościowe, które będziemy wykorzystywać później w MDS i PCA:

```{r}
zmienne_num <- c("GDP....per.capita.",
                 "Infant.mortality..per.1000.births.",
                 "Literacy....",
                 "Phones..per.1000.",
                 "Birthrate",
                 "Deathrate",
                 "Agriculture",
                 "Industry",
                 "Service")
```

W oryginalnym pliku wartości liczbowe są często zapisane z przecinkiem jako separatorem dziesiętnym (np. "48,0"). Zamieniamy przecinki na kropki, konwertujemy na typ numeryczny i wyrzucamy kraje z brakami w tych zmiennych.

```{r}
countries_clean <- countries_raw %>%
mutate(across(all_of(zmienne_num),
~ as.numeric(gsub(",", ".", .x, fixed = TRUE)))) %>%
filter(if_all(all_of(zmienne_num), ~ !is.na(.))) # wyrzucamy rozwalone wiersze
```

Sprawdzamy, ile krajów zostało po czyszczeniu

```{r}
nrow(countries_clean)
```

Po oczyszczeniu zbioru pozostaje 194 krajów, dla których mamy komplet informacji w wybranych zmiennych.

### 2.6.Wyznaczenie podstawowych statystyk opisowych

Najpierw obliczamy podstawowe statystyki opisowe dla wybranych zmiennych ilościowych, już po oczyszczeniu danych.

```{r}
# Podstawowe statystyki opisowe (min, max, median, mean itp.)

summary(countries_clean[ , zmienne_num])
```

```{r}
# Dokładniejsze statystyki (średnia, sd, min, max itd.)

psych::describe(countries_clean[ , zmienne_num])
```


### 2.7. Sprawdzenie, dla których krajów zmienne przyjmują wartości maksymalne

W tym kroku chcemy zobaczyć **kraje-rekordzistów**, czyli dla każdej wybranej zmiennej (np. PKB na mieszkańca, śmiertelność niemowląt, poziom alfabetyzmu) sprawdzamy, w którym kraju jej wartość jest **największa**. Dzięki temu łatwiej zrozumieć, jak wygląda "górny koniec skali" dla każdej cechy, czyli które państwa są skrajnie bogate, mają najwyższą śmiertelność, najwyższy udział usług itd. To daje szybki obraz ekstremów w zbiorze danych i pomaga później przy interpretacji wyników MDS i PCA.

```{r}
max_values <- countries_clean %>%
dplyr::select(Country, all_of(zmienne_num)) %>%
tidyr::pivot_longer(cols = all_of(zmienne_num),
names_to = "zmienna",
values_to = "wartosc_max") %>%
dplyr::group_by(zmienna) %>%
dplyr::slice(which.max(wartosc_max)) %>%
dplyr::arrange(zmienna) %>%
dplyr::select(zmienna, Country, wartosc_max)

knitr::kable(max_values,
caption = "Kraje z maksymalnymi wartościami wybranych zmiennych")
```


### 2.8. Sprawdzenie wartości minimalnych

Postępujemy analogicznie, tym razem dla wartości minimalnych. Wybieramy kraj, w którym dana zmienna przyjmuje najmniejszą wartość.

```{r}
min_values <- countries_clean %>%
dplyr::select(Country, all_of(zmienne_num)) %>%
tidyr::pivot_longer(cols = all_of(zmienne_num),
names_to = "zmienna",
values_to = "wartosc_min") %>%
dplyr::group_by(zmienna) %>%
dplyr::slice(which.min(wartosc_min)) %>%
dplyr::arrange(zmienna) %>%
dplyr::select(zmienna, Country, wartosc_min)

knitr::kable(min_values,
caption = "Kraje z minimalnymi wartościami wybranych zmiennych")
```

Na podstawie tabel z wartościami maksymalnymi i minimalnymi widać, że badane kraje bardzo silnie różnią się pod względem poziomu rozwoju. Przykładowo, najwyższy PKB na mieszkańca ma **Luksemburg** (55 100 USD), a najniższy **Sierra Leone** (500 USD). Najniższą śmiertelność niemowląt notuje **Singapur** (2,29), a najwyższą **Angola** (191,19). Najwyższy poziom alfabetyzmu (100%) ma **Australia**, podczas gdy najniższy ma **Niger** (17,6%). 

Widzimy też duże różnice w strukturze gospodarki: udział rolnictwa jest prawie zerowy w **Singapurze**, a bardzo wysoki w **Liberii**, natomiast udział usług jest największy na **Cayman Islands**, a najmniejszy w **Equatorial Guinea**. Zmienność dotyczy także wskaźników demograficznych (urodzenia, zgony). 


### 2.9.Sprawdzenie braków danych po czyszczeniu

Na koniec upewniamy się, że w danych używanych dalej (countries_clean) nie ma braków w wybranych zmiennych numerycznych.

```{r}
md.pattern(countries_clean[ , zmienne_num], rotate.names = TRUE)
```
Brak braków.

### 2.10. Przygotowanie danych do PCA i MDS

Teraz musimy przygotować dane tak, żeby dało się je włożyć do PCA i MDS.

- **PCA** działa na **tabeli z samymi liczbami** (kraje w wierszach, wybrane zmienne w kolumnach).  
- **MDS** działa na **tabeli odległości między parami krajów** (mówi, jak bardzo kraje są do siebie podobne lub różne).

Nasze zmienne są w różnych jednostkach (dolary, procenty, ułamki od 0 do 1). Gdybyśmy zostawili je tak jak są, to zmienne z dużymi liczbami (np. PKB) miałyby o wiele większy wpływ na wynik niż zmienne z małymi liczbami (np. udział rolnictwa). Dlatego najpierw będziemy **standaryzować** zmienne funkcją `scale()` w R. Dzięki temu:

- każda zmienna ma średnią równą 0,
- każda zmienna ma odchylenie standardowe 1,
- żadna zmienna nie dominuje tylko dlatego, że ma większe liczby.

Kolejne kroki są takie:

1. **Tworzymy macierz danych do PCA** – bierzemy z danych tylko wybrane zmienne liczbowe (`countries_pca_raw`).  
2. **Standaryzujemy te zmienne** – dostajemy `countries_pca_scaled`. Tego użyjemy później w PCA.  
3. **Na tych zestandaryzowanych danych liczymy odległości między krajami** – obiekt `dist_countries`. Tego użyjemy później w MDS (klasycznym i Sammona).

```{r}
# Macierz danych do PCA (wartości surowe)
countries_pca_raw <- countries_clean[ , zmienne_num]
```

```{r}
# Macierz danych do PCA (wartości standaryzowane)
countries_pca_scaled <- scale(countries_pca_raw)
```

```{r}
# Macierz odległości euklidesowych dla krajów (na danych standaryzowanych)
dist_countries <- dist(countries_pca_scaled, method = "euclidean")
```


## 3. Eksploracyjna analiza danych (EDA)

W tej części chcemy obejrzeć dane, zanim użyjemy bardziej skomplikowanych metod (MDS, PCA). Sprawdzimy:

- jak wyglądają rozkłady wybranych zmiennych (histogramy, boxploty),
- jak silnie zmienne są ze sobą powiązane (macierz korelacji),
- czy PCA ma sens, do tego użyjemy testu Bartletta na macierzy korelacji.

### 3.1. Rozkłady wybranych zmiennych – histogramy

Poniżej rysujemy histogramy dla wszystkich zmiennych z wektora `zmienne_num` (PKB, śmiertelność, alfabetyzacja, telefony, Birthrate, Deathrate, Agriculture, Industry, Service).

```{r}
countries_clean %>%
dplyr::select(all_of(zmienne_num)) %>%
tidyr::pivot_longer(cols = dplyr::everything(),
names_to = "zmienna",
values_to = "wartosc") %>%
ggplot2::ggplot(ggplot2::aes(x = wartosc)) +
ggplot2::geom_histogram(bins = 20, color = "white") +
ggplot2::facet_wrap(~ zmienna, scales = "free_x") +
ggplot2::theme_minimal() +
ggplot2::labs(title = "Histogramy wybranych zmiennych",
x = "wartość",
y = "liczba krajów")

```

Na histogramach widać, że:

- **GDP....per.capita.** – większość krajów ma raczej niskie PKB, a tylko kilka jest bardzo bogatych (silna skośność w prawo).
- **Infant.mortality..per.1000.births.**, **Birthrate**, **Deathrate** – wiele krajów ma niskie lub średnie wartości, ale pojawia się też grupa państw ze skrajnie wysokimi wartościami.
- **Literacy....** – większość krajów ma wysoki poziom alfabetyzmu, tylko nieliczne są bardzo nisko.
- **Phones..per.1000.** – przeważają kraje z umiarkowaną liczbą telefonów, ale są też państwa z bardzo dużą liczbą.
- **Agriculture**, **Industry**, **Service** – rozkłady pokazują duże zróżnicowanie udziału rolnictwa, przemysłu i usług w gospodarce.

Wniosek: w danych mamy zarówno kraje "typowe", jak i kilka bardzo skrajnych. W kolejnych częściach (MDS i PCA) sprawdzimy, jak te różnice między krajami będą widoczne na wykresach.


### 3.2. Wykresy pudełkowe (boxploty) dla wszystkich zmiennych

Teraz rysujemy boxploty dla tych samych zmiennych, żeby lepiej zobaczyć mediany, rozrzut i wartości odstające.

```{r}
countries_clean %>%
dplyr::select(all_of(zmienne_num)) %>%
tidyr::pivot_longer(cols = dplyr::everything(),
names_to = "zmienna",
values_to = "wartosc") %>%
ggplot2::ggplot(ggplot2::aes(x = zmienna, y = wartosc)) +
ggplot2::geom_boxplot() +
ggplot2::theme_minimal() +
ggplot2::coord_flip() +
ggplot2::labs(title = "Wykresy pudełkowe wybranych zmiennych",
x = "zmienna",
y = "wartość")

```

Na wykresach pudełkowych widać, że:

- **GDP....per.capita.** ma bardzo duży rozrzut i kilka punktów daleko na prawo, to kraje o wyjątkowo wysokim PKB na mieszkańca.
- Dla pozostałych zmiennych (śmiertelność niemowląt, alfabetyzm, telefony, Birthrate, Deathrate, Agriculture, Industry, Service) pudełka są dużo „węższe” na wspólnej osi, a wartości leżą blisko zera w porównaniu z PKB.
- Różnice w skali są ogromne – PKB jest liczone w tysiącach dolarów, a np. Agriculture, Industry i Service to udziały między 0 a 1.

Wniosek: zmienne mają bardzo różne skale i pojawiają się wartości odstające, dlatego **konieczna jest standaryzacja danych** przed zastosowaniem MDS i PCA.


### 3.3. Korelacje między zmiennymi

PCA opiera się na tym, jak **zmienne są ze sobą powiązane**. Dlatego najpierw liczymy **macierz korelacji** dla wszystkich wybranych zmiennych (`zmienne_num`).

 - wartości blisko 1 → silna dodatnia zależność,
 - blisko -1 → silna ujemna,
 - blisko 0 → brak silnej zależności liniowej.

```{r}
# Macierz korelacji Pearsona dla wybranych zmiennych
cor_matrix <- cor(countries_clean[ , zmienne_num],
                  use = "complete.obs",
                  method = "pearson")

# Zaokrąglona macierz korelacji do dwóch po przecinku
round(cor_matrix, 2)
```


```{r}
corrplot(cor_matrix,
         method = "color",
         type   = "upper",
         tl.col = "black",
         tl.srt = 45,
         addCoef.col = "black",   # dodaje wartości korelacji na kafelkach
         number.cex = 0.7)

```
Wykres:

- kolory blisko 1 (ciemne) oznaczają silną dodatnią korelację – gdy jedna zmienna rośnie, druga też rośnie,
 - kolory blisko -1 (niebieskie) oznaczają silną ujemną korelację – gdy jedna zmienna rośnie, druga spada,
 - wartości blisko 0 oznaczają słabą lub brak zależności liniowej.

Z macierzy korelacji widać kilka prostych zależności:

- Kraje o **wyższym PKB na mieszkańca** mają zwykle **więcej telefonów**, **wyższy poziom alfabetyzmu**, **niższą śmiertelność niemowląt** oraz **mniejszy udział rolnictwa**, a **większy udział usług** w gospodarce.  
- **Wyższa śmiertelność niemowląt** idzie w parze z **wyższym współczynnikiem urodzeń**, większym udziałem **rolnictwa** oraz niższym **alfabetyzmem** i mniejszym udziałem **usług**.  
- **Wyższy poziom alfabetyzmu** wiąże się z **niższym Birthrate**, mniejszym udziałem **rolnictwa** i większym udziałem **usług**.  
- Zmienna **Industry** jest najsłabiej skorelowana z pozostałymi wskaźnikami, więc jej wpływ w PCA może być mniej widoczny (PCA ma sens tylko, gdy dane są w istotnym stopniu skorelowane).

W skrócie: bogatsze, bardziej „usługowe” kraje są lepiej wykształcone, mają lepszy dostęp do technologii i niższą śmiertelność niemowląt, a biedniejsze i bardziej „rolnicze” – wyższy Birthrate i gorsze wskaźniki zdrowotne.


### 3.3. Test Bartletta – czy PCA ma sens?

Test Bartletta służy do sprawdzenia, czy macierz korelacji wskazuje na wystarczające powiązanie zmiennych (hipoteza
zerowa tego testu zakłada, że zmienne nie są ze sobą dostatecznie powiązane).

PCA ma sens tylko wtedy, gdy między zmiennymi istnieją **jakieś korelacje**. Żeby to formalnie sprawdzić, używamy **testu Bartletta** (funkcja cortest.bartlett()z pakietu `psych`).

- \(H_0\): zmienne prawie się nie korelują (macierz korelacji ≈ macierz jednostkowa) – PCA nie ma sensu,
- \(H_1\): zmienne są istotnie skorelowane – można stosować PCA.

```{r}
# Test Bartletta na macierzy korelacji
bartlett_result <- psych::cortest.bartlett(cor_matrix,
                                           n = nrow(countries_clean))
bartlett_result
```
W teście Bartletta otrzymaliśmy:

- statystykę chi-kwadrat ≈ 2475,
- liczbę stopni swobody df = 36,
- p-value ≈ 0 (praktycznie 0, znacznie poniżej 0.05).

Tak małe p-value oznacza, że **odrzucamy hipotezę zerową** mówiącą, że zmienne są nieskorelowane (macierz korelacji jest jak macierz jednostkowa).

Wniosek: zmienne są istotnie skorelowane, więc zastosowanie PCA na tym zbiorze danych jest uzasadnione.

### 3.4. Dodatkowe wykresy punktowe

Na koniec EDA rysujemy dwa proste wykresy punktowe, żeby zobaczyć zależności między wybranymi parami zmiennych.

```{r}
# PKB na mieszkańca a śmiertelność niemowląt

countries_clean %>%
  ggplot(aes(x = GDP....per.capita.,
             y = Infant.mortality..per.1000.births.,
             color = Region)) +
  geom_point() +
  theme_minimal() +
  labs(title = "PKB na mieszkańca a śmiertelność niemowląt",
       x = "GDP ($ per capita)",
       y = "Infant mortality (per 1000 births)",
       color = "Region")
```

Na wykresie widać wyraźnie, że kraje o wyższym PKB na mieszkańca mają zwykle niższą śmiertelność niemowląt. Kraje biedniejsze są skupione w lewym dole wykresu (niski PKB, wysoka śmiertelność), a bogatsze po prawej, z niską śmiertelnością.

```{r}
# PKB na mieszkańca a poziom alfabetyzmu

countries_clean %>%
ggplot(aes(x = GDP....per.capita.,
y = Literacy....,
color = Region)) +
geom_point() +
theme_minimal() +
labs(title = "PKB na mieszkańca a poziom alfabetyzmu",
x = "GDP ($ per capita)",
y = "Literacy (%)",
color = "Region")

```

Tutaj widać, że wyższy PKB na mieszkańca zwykle wiąże się z wyższym poziomem alfabetyzmu. Bardzo biedne kraje mają często niski odsetek osób umiejących czytać i pisać, a kraje bogate są skupione w górnej części wykresu (wysoki PKB, wysoki poziom alfabetyzmu).

Te wykresy potwierdzają wnioski z macierzy korelacji i pokazują zależności między krajami bardziej "intuicyjnie" na płaszczyźnie.


## 4. Skalowanie wielowymiarowe (MDS)

Metoda pozwalająca zwizualizować obiekty n-wymiarowe w przestrzeni m-wymiarowej (m<n). Celem MDS jest "spłaszczenie" naszych danych tak, żeby każdy kraj był punktem na 2D, a odległości między punktami były możliwie podobne do prawdziwych różnic między krajami (liczonych na podstawie naszych wybranych skaźników: PKB, śmiertelność, itp.).

Na tym etapie mamy już:

 - wybrane zmienne liczbowe (zmienne_num),
 - wyczyszczone dane (countries_clean),
 - dane zestandaryzowane (countries_pca_scaled),
 - macierz odległości euklidesowych między krajami (dist_countries).

Teraz wykorzystamy te odległości w dwóch metodach MDS:

 - klasyczne skalowanie wielowymiarowe (cmdscale),
 - metodę skalowania Sammona (będziemy ją wywoływać jako `MASS::sammon(...)`, więc nie musimy robić `library(MASS)`).
 
Funkcja `sammon()` z pakietu MASS ma postać `sammon(d, k, ...)`, gdzie:

- `d` – macierz odległości / macierz niepodobieństw,
- `k` – wymiar przestrzeni, w której mają być reprezentowane dane.

### 4.1. Miara odległości między krajami

MDS nie pracuje bezpośrednio na oryginalnych zmiennych, tylko na odległościach między parami obiektów (u nas: krajów). Dlatego wcześniej (w pkt 2.10) policzyliśmy macierz odległości euklidesowych na danych zestandaryzowanych:

 - używamy odległości euklidesowej – zwykła "odległość w linii prostej" między dwoma punktami (krajami) w przestrzeni wielu zmiennych,
 - dane są zestandaryzowane, żeby żadna zmienna (np. PKB) nie dominowała tylko dlatego, że ma większe liczby.

Nasza macierz odległości:

```{r}
# Macierz odległości euklidesowych dla krajów (na danych standaryzowanych)
dist_countries <- dist(countries_pca_scaled, method = "euclidean")

# (opcjonalnie) zamiana na zwykłą macierz do podglądu
dist_matrix <- as.matrix(dist_countries)
dist_matrix[1:5, 1:5]
```
Dla kontroli wyświetlamy fragment macierzy odległości (pierwsze 5 krajów z 194 naszych). Każdy element tej macierzy to:

 - 0 na przekątnej – odległość kraju od samego siebie,
 - liczba dodatnia poza przekątną – im większa, tym dwa kraje są bardziej różne pod względem wszystkich wskaźników naraz.
 
Na tej macierzy będziemy teraz uruchamiać algorytmy MDS.

Dla przykładu (im większa liczba, tym bardziej różne kraje):

 - kraj 1 i 4 są dość podobne (3.8),
 - kraj 1 i 5 mocno się różnią (7.5),
 - kraj 2 i 5 są raczej podobne (2.6).

### 4.2. Klasyczne skalowanie wielowymiarowe (metryczne MDS)

Klasyczne MDS robi tak:

 - bierze odległości między krajami (dist_countries),
 - szuka punktów na płaszczyźnie (2D), tak żeby odległości między punktami były możliwie podobne do odległości w oryginalnych danych,
 - jest metodą metryczną (korzysta z wartości odległości) i liniową (wynik dostajemy z obliczeń macierzowych, bez iteracji).

Jako miarę jakości rzutowania często używa się współczynnika STRESS. W którym:

 - dik - odległość między krajem i i k przed MDS (z dist_countries (wyznaczona na podstwie początkowych danych)),
 - d^ik - odległość między tymi samymi krajami po rzutowaniu do 2D.

Im mniejszy STRESS (bliżej 0), tym lepsze odwzorowanie:

 - STRESS < 0.10 – bardzo dobre odwzorowanie,
 - ok. 0.10–0.20 – w miarę OK, ale są zniekształcenia,
 - mniejsze od 0.20 – rzutowanie 2D mocno upraszcza odległości.
 
#### 4.2.1. Uruchomienie cmdscale i wykres
```{r}
# Klasyczne MDS (cmdscale) w 2 wymiarach

mds_cmd <- cmdscale(dist_countries,   # macierz odległości
k   = 2,          # chcemy 2 wymiary
eig = TRUE)       # zapisz też wartości własne

# Współrzędne krajów w 2D

mds_cmd_points <- as.data.frame(mds_cmd$points)
colnames(mds_cmd_points) <- c("Dim1", "Dim2")  # 2 wymiary

# Dodajemy nazwy krajów i regiony (z oryginalnych danych)

mds_cmd_points$Country <- countries_clean$Country
mds_cmd_points$Region  <- countries_clean$Region
```

Wykres gdzie lepiej widać zgrupowania krajów.
```{r}
# Wykres klasycznego MDS – etykiety size = 2 (lepsza czytelność grup)

p_size2 <- ggplot(mds_cmd_points,
aes(x = Dim1, y = Dim2, color = Region)) +
geom_point() +
geom_text(aes(label = Country),
check_overlap = TRUE, size = 2) +
theme_minimal() +
labs(title = "Klasyczne skalowanie wielowymiarowe (cmdscale)",
x = "Wymiar 1",
y = "Wymiar 2",
color = "Region")

p_size2
```

Wykres gdzie lepiej widać nazwy krajów.
```{r}
# wersja z rozmiarem etykiet = 3

p_size3 <- ggplot(mds_cmd_points,
aes(x = Dim1, y = Dim2, color = Region)) +
geom_point() +
geom_text(aes(label = Country),
check_overlap = TRUE, size = 3) +
theme_minimal() +
labs(title = "cmdscale – etykiety size = 3",
x = "Wymiar 1",
y = "Wymiar 2",
color = "Region")

p_size3
```

Na rysunku pokazano wynik klasycznego skalowania wielowymiarowego (cmdscale) w dwóch wymiarach. Każdy punkt to jeden kraj, a kolor oznacza region świata.

Widać, że:

- kraje z **wysokim poziomem rozwoju** (zwykle Europa Zachodnia, Ameryka Północna, część Oceanii) są skupione w jednej części wykresu – mają podobne wartości PKB, wysoką alfabetyzację, niski poziom śmiertelności niemowląt itp.;
- kraje **biedniejsze, bardziej rolnicze** (głównie **Sub-Saharan Africa** i część regionów azjatyckich) tworzą osobne skupienia w innej części wykresu;
- regiony pośrednie (np. część krajów Ameryki Łacińskiej, Europy Wschodniej) leżą „pomiędzy” tymi skrajnościami.

Oznacza to, że klasyczne MDS potrafi sensownie ułożyć kraje na płaszczyźnie: państwa o podobnych wskaźnikach społeczno-gospodarczych leżą blisko siebie, a kraje bardzo różne znajdują się daleko od siebie.


#### 4.2.2. STRESS dla klasycznego MDS
```{r}
# Odległości oryginalne (na wszystkich zmiennych standaryzowanych)

d_orig <- as.numeric(dist_countries)

# Odległości po rzutowaniu do 2D (cmdscale)

d_mds  <- as.numeric(dist(mds_cmd_points[ , c("Dim1", "Dim2")]))

# Współczynnik STRESS (z wzoru STRESS)

stress_cmd <- sqrt( sum( (d_orig - d_mds)^2 ) / sum(d_orig^2) )
stress_cmd
```

```{r}
# MDS w 3 wymiarach

mds_cmd3 <- cmdscale(dist_countries, k = 3, eig = TRUE)

mds_cmd3_points <- as.data.frame(mds_cmd3$points)
colnames(mds_cmd3_points) <- c("Dim1", "Dim2", "Dim3")

# STRESS 3D

d_orig <- as.numeric(dist_countries)
d_mds3 <- as.numeric(dist(mds_cmd3_points[ , c("Dim1", "Dim2", "Dim3")]))

# Współczynnik STRESS (z wzoru STRESS)

stress_cmd3 <- sqrt( sum( (d_orig - d_mds3)^2 ) / sum(d_orig^2) )
stress_cmd3
```

Dla rzutowania 2-wymiarowego otrzymaliśmy STRESS ≈ 0.21, natomiast dla 3 wymiarów STRESS ≈ 0.13. Oznacza to, że reprezentacja 3D lepiej odwzorowuje odległości między krajami, ale w dalszej części pracy korzystamy głównie z rzutowania 2D, bo jest łatwiejsze do interpretacji graficznej.

### 4.3. Metoda skalowania Sammona

#### 4.3.1. Krótki opis metody

Metoda Sammona to druga wersja MDS, w której:

 - dalej używamy odległości metrycznych (np. euklidesowych),
 - kładzie większy nacisk na dobre odwzorowanie małych odległości, czyli bardziej dba o to, żeby kraje podobne pozostały blisko siebie (odpowiedni dobór wag),
 - jest to metoda nieliniowa i iteracyjna, startuje z pewnego rozwiązania i krok po kroku je poprawia,
 - rozwiązywany jest problem optymalizacyjny, w którym minimalizujemy funkcję błędu (tzw. błąd Sammona).
 - Im mniejszy ten błąd, tym lepsze dopasowanie rzutowania (Im mniejsze 
E, tym lepsze dopasowanie)

W skrócie: klasyczny MDS stara się dobrze odwzorować wszystkie odległości, a Sammon „bardziej dba” o sąsiadów.

#### 4.3.2. Zastosowanie w R – sammon

Podajemy tę samą macierz odległości dist_countries, co w klasycznym MDS, i rzutujemy do 2 wymiarów. Używamy funkcji sammon() z pakietu MASS sammon(d, k, ...) na tej samej macierzy odległości dist_countries, gdzie:

    d - macierz odległości/macierz niepodobieństw
    k - maksymalny wymiar przestrzeni, w której mają być reprezentowane dane

```{r}
set.seed(123)  # dla powtarzalności wyniku

mds_sammon <- MASS::sammon(dist_countries, # macierz odległości
                           k = 2)  # 2 wymiary

# Współrzędne w 2D

mds_sammon_points <- as.data.frame(mds_sammon$points)
colnames(mds_sammon_points) <- c("Dim1", "Dim2") # 2 wymiary

mds_sammon_points$Country <- countries_clean$Country
mds_sammon_points$Region  <- countries_clean$Region
```

Wizualizacja rozwiązania Sammona:

```{r}
ggplot(mds_sammon_points,
aes(x = Dim1, y = Dim2, color = Region)) +
geom_point() +
geom_text(aes(label = Country),
check_overlap = TRUE, size = 2) +
theme_minimal() +
labs(title = "Metoda skalowania Sammona",
x = "Wymiar 1",
y = "Wymiar 2",
color = "Region")

```

Wykres z metody Sammona pokazuje kraje jako punkty w 2D. Kraje o podobnych wskaźnikach (PKB, śmiertelność, rolnictwo, usługi itd.) leżą blisko siebie, a bardzo różne – daleko od siebie. Widać wyraźne grupy regionów (Europa, Afryka, Ameryka itd.), a pojedyncze punkty „na boku” to kraje odstające od reszty. Układ jest podobny jak w klasycznym MDS ( u nas w czśći 4.2.1), ale Sammona lepiej widać małe różnice między podobnymi krajami.

#### 4.3.3. Jakość rzutowania – STRESS Sammona

Funkcja sammon() sama zwraca wartość błędu (STRESS Sammona) w elemencie stress. Im mniejszy ten błąd, tym lepsze dopasowanie rzutowania 2D do prawdziwych odległości między krajami.

```{r}
# STRESS dla metody Sammona

sammon_stress <- mds_sammon$stress
sammon_stress
```

Dla metody Sammona otrzymaliśmy STRESS ≈ 0,04. Jest to dużo mniej niż dla klasycznego MDS w 2D (≈ 0,21) i 3D (≈ 0,13). Oznacza to, że rzutowanie Sammona lepiej zachowuje odległości między krajami, zwłaszcza małe odległości (kraje podobne są dobrze trzymane blisko siebie).


### 4.4. Porównanie wyników MDS

Na koniec porównujemy wszystkie zastosowane metody:

 - klasyczne MDS w 2D (cmdscale, STRESS = stress_cmd),
 - klasyczne MDS w 3D (cmdscale, STRESS = stress_cmd3),
 - metoda Sammona w 2D (sammon, STRESS = mds_sammon$stress).

```{r}
porownanie_stress <- data.frame(
Metoda = c("cmdscale 2D", "cmdscale 3D", "Sammon 2D"),
STRESS = c(stress_cmd, stress_cmd3, sammon_stress)
)

knitr::kable(porownanie_stress,
caption = "Porównanie wartości STRESS dla różnych metod MDS")

```

W tabeli widać, że największy błąd (STRESS) ma klasyczny MDS w 2D (≈ 0,21), mniejszy w 3D (≈ 0,13), a najmniejszy błąd ma metoda Sammona w 2D (≈ 0,04). Im mniejszy STRESS, tym lepsze odwzorowanie odległości między krajami, więc Sammona daje najlepsze dopasowanie, potem cmdscale 3D, a najsłabszy jest cmdscale 2D.


### 4.5. Wnioski i krótkie podsumowanie metod MDS 

Na wszystkich wykresach podobne kraje są blisko siebie, a bardzo różne – daleko (bogate vs biedne, rolnicze vs usługowe).

- **cmdscale 2D** działa OK, ale STRESS ≈ 0,21 oznacza, że rzut 2D dość mocno zniekształca odległości.  
- **cmdscale 3D** ma mniejszy błąd (≈ 0,13), więc lepiej trzyma odległości, ale trudniej taki wynik ładnie pokazać na wykresie.  
- **Sammon 2D** ma STRESS ≈ 0,04 – najlepszą jakość z naszych metod; szczególnie dobrze pokazuje małe różnice między podobnymi krajami.

Na wykresie Sammona grupy krajów są bardziej „rozciągnięte”, więc lepiej widać różnice wewnątrz regionów, podczas gdy cmdscale 2D pokazuje ogólny układ, ale część krajów bardziej się na siebie nakłada.

**Wady i zalety metod:**

- **Klasyczne MDS (cmdscale)**  
  + zalety: prosta, szybka metoda, łatwa do policzenia nawet dla dużej liczby krajów; wynik jest jednoznaczny (brak iteracji),  
  + wady: w 2D gorzej odwzorowuje odległości (dość duży STRESS), więc rzutowanie jest „spłaszczone”.

- **Metoda Sammona**  
  + zalety: lepiej zachowuje małe odległości (lokalne podobieństwa), w naszych danych ma zdecydowanie mniejszy STRESS, więc lepiej oddaje relacje między krajami,  
  + wady: metoda jest wolniejsza (iteracyjna), wynik zależy od punktu startowego i może zatrzymać się w rozwiązaniu tylko lokalnie najlepszym.

Ogólnie: układ grup krajów jest podobny w cmdscale i w Sammonie, ale metoda Sammona daje dokładniejsze odwzorowanie odległości między krajami.



## 5. Analiza składowych głównych (PCA)

PCA (Principal Component Analysis) to metoda zmniejszania wymiaru danych. Czyli, Zalezienie nowego układu współrzędnych, czyli składowych głównych będących liniowymi kombinacjami oryginalnych zmiennych, które będą wyjaśniały jak największą
część zmienności w danych.

(Zamiast 10 podobnych do siebie kolumn robimy np. 2–3 "super-kolumny", które prawie wszystko streszczają. Warunek: stare zmienne muszą być jakoś ze sobą powiązane (skorelowane). Jak wszystko byłoby całkiem niezależne, to nie dałoby się tego sensownie "upchnąć" w mniejszą liczbę super-zmiennych.)

### 5.1. Założenia i wybór macierzy (kowariancja vs korelacja)

Z prezentacji:

- gdy zmienne są w podobnej skali → można użyć **macierzy kowariancji**,
- gdy skale są bardzo różne → lepiej użyć **macierzy korelacji po standaryzacji**.

W naszych danych o krajach zmienne mają bardzo różne skale (PKB na mieszkańca, % w rolnictwie, śmiertelność itd.), dlatego:

- najpierw dane **standaryzujemy** (to już zrobiliśmy –> obiekt `countries_pca_scaled`),
- **PCA wykonujemy na macierzy korelacji** między naszymi zmiennymi.


```{r}
# Dane do PCA – standaryzowane zmienne liczbowe
X_pca <- as.matrix(countries_pca_scaled)

# Macierz korelacji zmiennych
cor_mat <- cor(X_pca)

# Podgląd fragmentu macierzy korelacji
cor_mat[1:5, 1:5]
```

### 5.2. Wyznaczenie PCA różnymi funkcjami

W prezentacji były pokazane różne funkcje do PCA: eigen(), svd(), prcomp(), princomp() (z biblioteki base).

U nas użyjemy dwóch podejść i porównamy wyniki:

#### 5.2.1. Metoda 1 – eigen() na macierzy korelacji

Najpierw liczymy macierz korelacji dla zestandaryzowanych zmiennych, a potem używamy funkcji eigen():

 - eigen() zwraca wartości własne (ile wariancji "niesie" każda składowa)
oraz wektory własne (ładunki – udział każdej zmiennej w danej składowej).
 - Na tej podstawie obliczamy wartości składowych głównych (scores) jako iloczyn: zestandaryzowane dane × wektory własne.

Czyli w tej podsekcji realizujemy pierwszą metodę ze slajdów – PCA z macierzy korelacji za pomocą eigen().

Krok 1: liczymy wartości własne i wektory własne macierzy korelacji:
```{r}
# PCA z użyciem eigen() – na macierzy korelacji
eigen_pca <- eigen(cor_mat)

# Funkcja eigen() dla macierzy symetrycznych (jak macierz korelacji)
# domyślnie sortuje wartości własne malejąco.
# Jest to kluczowe dla PCA, gdzie PC1 musi wyjaśniać najwięcej wariancji.
eigen_values <- eigen_pca$values
eigen_values

# Wektory własne (ładunki) odpowiadają kolejnym wartościom własnym
eigen_vectors <- eigen_pca$vectors
```

Krok 2: z wektorów własnych liczymy wartości składowych głównych (scores):
```{r}
# Wartości składowych głównych (scores) obliczone "ręcznie"
scores_eigen <- X_pca %*% eigen_vectors
scores_eigen <- as.data.frame(scores_eigen)

# Nazwy składowych
colnames(scores_eigen) <- paste0("PC", 1:ncol(scores_eigen))

# Dodajemy kraj i region
scores_eigen$Country <- countries_clean$Country
scores_eigen$Region  <- countries_clean$Region

head(scores_eigen)
```


 - eigen_values – to wartości własne (ile wariancji wyjaśnia każda PC),
 - eigen_vectors – to ładunki (jaki jest udział zmiennych w każdej PC),
 - scores_eigen – to położenie krajów w przestrzeni składowych (PC1, PC2, …).
 
#### 5.2.2. Metoda 2 – prcomp() (SVD)

Drugi raz liczymy PCA funkcją prcomp() z opcjami center = TRUE, scale. = TRUE. Ta funkcja wykorzystuje rozkład SVD macierzy danych (druga metoda ze slajdów).

prcomp() zwraca odchylenia standardowe składowych (sdev), po podniesieniu do kwadratu (sdev^2) dostajemy wartości własne.

```{r}
# PCA z użyciem prcomp() – na tych samych danych

pca_prcomp <- prcomp(countries_clean[ , zmienne_num],
center = TRUE,   # odejmujemy średnie
scale. = TRUE)   # standaryzacja -> macierz korelacji

summary(pca_prcomp)

```
W pca_prcomp mamy m.in.:

 - pca_prcomp$sdev – odchylenia standardowe składowych,
 - pca_prcomp$rotation – ładunki (wektory własne),
 - pca_prcomp$x – wartości składowych (scores).

Porównanie wartości własnych:
```{r}
# Wartości własne z prcomp() (sdev^2)

eigen_from_prcomp <- pca_prcomp$sdev^2
eigen_from_prcomp

# Porównanie z wartościami z eigen()

cbind(eigen_eigen  = eigen_values,
eigen_prcomp = eigen_from_prcomp)
```

Porównując wartości własne z eigen() i z prcomp() dostajemy identyczne liczby. To potwierdza teorię z prezentacji: PCA policzone z macierzy korelacji i PCA policzone przez SVD dają te same wartości własne (różni się tylko sposób obliczeń, nie wynik).


### 5.3. Analiza wartości własnych i wybór liczby składowych

Najpierw zrobimy tabelę z wartościami własnymi i procentem wyjaśnionej wariancji.
```{r}
# Tabela: wartości własne i procent wariancji

p <- length(eigen_values)               # liczba składowych = liczba zmiennych
prop_var <- eigen_values / sum(eigen_values)   # udział wariancji
cum_var  <- cumsum(prop_var)                   # wariancja skumulowana

eigen_table <- data.frame(
PC        = paste0("PC", 1:p),
Eigenvalue = round(eigen_values, 3),
Proportion = round(prop_var, 3),
Cumulative = round(cum_var, 3)
)

knitr::kable(eigen_table,
caption = "Wartości własne i procent wyjaśnionej wariancji (PCA)")

```

Z tabeli widać, że:

 - PC1 ma największą wartość własną (5.26) i sama wyjaśnia ok. 58% zmienności w danych.
 - PC1 + PC2 razem dają ok. 74% wariancji,
 - PC1 + PC2 + PC3 – już ok. 85%.

Kolejne składowe (PC4–PC9) dodają tylko po kilka procent, więc mają małe znaczenie. W dalszej analizie skupiamy się głównie na pierwszych 2–3 składowych, bo to one opisują prawie całą informację w danych.


#### 5.3.1. Scree plot (wykres osypiska)

Używamy funkcji fviz_eig() ze slajdów (pakiet factoextra):

```{r}
# Wykres osypiska (scree plot)

# (potrzebny pakiet factoextra)

factoextra::fviz_eig(pca_prcomp, addlabels = TRUE)
```

Na wykresie „scree plot” widać, jaki procent wariancji wyjaśnia każda składowa:

 - PC1 ≈ 58% wariancji,
 - PC2 ≈ 16%,
 - PC3 ≈ 10%,

kolejne składowe po kilka procent lub mniej. Łącznie:

 - PC1 + PC2 ≈ 74% wariancji,
 - PC1 + PC2 + PC3 ≈ 85% wariancji.

Po PC3 słupki są już małe i wykres się „wypłaszcza” – klasyczny efekt łokcia. Dlatego w dalszej analizie skupiamy się głównie na pierwszych 2–3 składowych głównych (PC1–PC3), bo one wyjaśniają zdecydowaną większość informacji w danych.


### 5.4. Interpretacja składowych głównych

Teraz patrzymy na ładunki (wektory własne) –> czyli jak każda zmienna "wchodzi" do danej składowej.
```{r}
# Ładunki (wektory własne) z prcomp()

loadings <- pca_prcomp$rotation

# Podgląd ładunków dla pierwszych 3 składowych

round(loadings[ , 1:3], 3)
```

PC1 – „poziom rozwoju kraju”:

 - duże +: PKB, piśmienność, telefony, usługi
 - duże –: śmiertelność niemowląt, urodzenia, rolnictwo
Wysoka PC1 = bogaty, „nowoczesny” kraj; niska PC1 = biedniejszy, rolniczy.

PC2 – „przemysł vs usługi”:

 - duże +: przemysł (Industry)
 - duże –: usługi (Service), trochę rolnictwo
Wysoka PC2 = kraj bardziej przemysłowy, niska PC2 = kraj bardziej usługowy.

PC3 – „śmiertelność / demografia”:

 - duże +: głównie śmiertelność ogólna (Deathrate)
Ta składowa odróżnia kraje o wyższej i niższej śmiertelności.


### 5.5. Wyniki PCA: konfiguracja krajów

#### 5.5.1. Wykres PC1–PC2
```{r}
# Współrzędne krajów w przestrzeni PC1–PC2 z prcomp()

scores_pc <- as.data.frame(pca_prcomp$x)
scores_pc$Country <- countries_clean$Country
scores_pc$Region  <- countries_clean$Region

ggplot(scores_pc, aes(x = PC1, y = PC2, color = Region)) +
geom_point() +
geom_text(aes(label = Country),
check_overlap = TRUE, size = 2) +
theme_minimal() +
labs(title = "Kraje w przestrzeni PC1–PC2",
x = "PC1",
y = "PC2",
color = "Region")

```

Interpretacja wykresu PC1–PC2:

Oś PC1 "poziom rozwoju kraju":

 - po prawej stronie: kraje bogatsze, z dużym udziałem usług, wysoką piśmiennością, dużą liczbą telefonów (np. Europa Zachodnia, Ameryka Północna),
 - po lewej stronie: kraje biedniejsze, bardziej rolnicze, z dużą śmiertelnością niemowląt (głównie Sub-Saharan Africa).

Oś PC2 "przemysł vs usługi”:

 - wyżej: kraje bardziej przemysłowe,
 - niżej: kraje bardziej usługowe.

Na wykresie widać znowu wyraźne grupy regionów, bardzo podobne jak w MDS:

 - kraje zachodnie skupione razem (prawo-dół),
 - wiele krajów afrykańskich po lewej,
 - kraje „pośrednie” (np. część Ameryki Łacińskiej, Europy Wschodniej) w środku.

Wniosek: PCA i MDS pokazują bardzo podobny obraz świata, kraje o podobnych wskaźnikach leżą blisko siebie, a różne są daleko. PCA dodatkowo mówi, jakie kombinacje zmiennych (PC1, PC2) za to odpowiadają.


#### 5.5.2. Biplot
```{r}
factoextra::fviz_pca_biplot(
pca_prcomp,
repel = TRUE,   # odsuwanie etykiet
label = "var"   # podpisujemy tylko zmienne (strzałki)
)
```

Na biplocie na osi poziomej (Dim1, ok. 58% wariancji) widać głównie **poziom rozwoju kraju**:

- strzałki w prawo: `GDP.per.capita`, `Phones.per.1000`, `Literacy`, `Service`  
  → kraje bogatsze, z większym udziałem usług, większą liczbą telefonów i wyższą edukacją,
- strzałki w lewo: `Infant.mortality`, `Birthrate`, `Agriculture`  
  → kraje biedniejsze, bardziej rolnicze, z wysoką śmiertelnością niemowląt i wysokim przyrostem naturalnym.

Oś pionowa (Dim2, ok. 16% wariancji) odróżnia **kraje bardziej przemysłowe**:

- strzałka w górę: `Industry` → wyższy udział przemysłu w gospodarce.

Długość strzałek pokazuje, które zmienne najmocniej wpływają na wynik (dłuższe strzałki = ważniejsze zmienne).

Podsumowanie:

- **prawo-dół** – kraje bogate, usługowe, dobrze rozwinięte,
- **lewo-góra** – kraje biedniejsze, rolniczo-przemysłowe,
- **środek wykresu** – kraje „średnie” pod względem analizowanych wskaźników.


### 5.6. Wnioski z 5 częśći

PCA pokazuje ten sam podział krajów co MDS: bogate usługowe kraje kontra biedniejsze rolnicze. Dodatkowo PCA mówi, że za główne różnice odpowiada kombinacja zmiennych związanych z poziomem rozwoju (PKB, usługi, telefony, śmiertelność). W dalszej części w podsumowaniu porównamy to z wynikami MDS.



## 6. Porównanie MDS i PCA

W tej części porównujemy wyniki skalowania wielowymiarowego (MDS) i analizy składowych głównych (PCA) dla tych samych danych o krajach.

### 6.1. Podobieństwa konfiguracji krajów

Na wykresach:

- MDS (zarówno **cmdscale 2D**, jak i **Sammon 2D**),
- PCA w przestrzeni **PC1–PC2**

widać bardzo podobny ogólny obraz:

- **po „prawej stronie” / w jednym rogu** leżą głównie kraje bogate, usługowe, z wysoką piśmiennością i dużą liczbą telefonów (Europa Zachodnia, Ameryka Północna, część Oceanii),
- **po „lewej stronie”** – kraje biedniejsze, bardziej rolnicze, z wysoką śmiertelnością niemowląt (głównie Sub-Saharan Africa),
- **kraje pośrednie** (część Ameryki Łacińskiej, Europy Wschodniej itd.) leżą mniej więcej pośrodku zarówno na wykresach MDS, jak i PCA.

Czyli: **kraje, które są blisko siebie na wykresach MDS, zazwyczaj są też blisko w przestrzeni PC1–PC2** – zwłaszcza gdy patrzymy na duże grupy (bogate vs biedne, rolnicze vs usługowe).

Dla ilustracji można policzyć korelacje między współrzędnymi z MDS i PCA:

```{r}
mds_pca_compare <- data.frame(
  PC1  = scores_pc$PC1,
  PC2  = scores_pc$PC2,
  MDS1 = mds_sammon_points$Dim1,
  MDS2 = mds_sammon_points$Dim2
)

round(cor(mds_pca_compare), 2)
```
(Przykładowy wynik u nas:)

 - PC1 ~ MDS1: korelacja ≈ -1.00
 - PC2 ~ MDS2: korelacja ≈ -0.93

Co z tego wynika:

 - PC1 i MDS1 pokazują prawie to samo, tylko oś jest odbita w lustrze (minus oznacza, że „prawa strona” zamienia się z „lewą”, ale układ krajów jest ten sam).
 - PC2 i MDS2 też pokazują prawie to samo.

W praktyce:

 - te same kraje są blisko siebie na wykresie MDS i na wykresie PC1–PC2,
 - bogate, usługowe kraje lądują razem, biedniejsze rolnicze też razem – niezależnie od tego, czy patrzymy na MDS, czy na PCA.
 
 
### 6.2. Co robi MDS, a co PCA?

MDS (skalowanie wielowymiarowe)

 - bierze odległości między krajami (policzone na podstawie wszystkich zmiennych),
 - rysuje punkty tak, żeby odległości na wykresie były jak najbardziej podobne do tych prawdziwych,
 - osie Dim1/Dim2 nie mają prostego znaczenia (ważne jest tylko „kto jest blisko kogo, a kto daleko").

PCA (analiza składowych głównych)

 - bierze korelacje między zmiennymi (GDP, śmiertelność, alfabetyzacja, telefony, rolnictwo, usługi itd.),
 - tworzy nowe zmienne: PC1, PC2, … – są to mieszanki starych zmiennych,

każda oś ma sens:

 - PC1 ≈ „poziom rozwoju kraju” (bogaty/usługowy vs biedny/rolniczy),
 - PC2 ≈ „przemysł vs usługi”.

Czyli:

 - MDS mówi: „te kraje są do siebie podobne / różne”,
 - PCA mówi dodatkowo: „bo różnią się głównie bogactwem, strukturą gospodarki itd.”.
 
 
### 6.3. Plusy i minusy metod w naszych danych

| Co porównujemy      | MDS (Sammon) – w naszym projekcie                               | PCA – w naszym projekcie                                                                 |
|---------------------|------------------------------------------------------------------|-------------------------------------------------------------------------------------------|
| **Dane wejściowe**  | Macierz odległości między krajami (na danych standaryzowanych). | Macierz korelacji między zmiennymi (po standaryzacji danych).                            |
| **Co najlepiej pokazuje** | Kto jest blisko/daleko – które kraje są podobne lub bardzo różne. | Główne osie różnic między krajami: poziom rozwoju, struktura gospodarki (rolnictwo/usługi/przemysł). |
| **Interpretacja osi** | Osie Dim1 i Dim2 nie mają prostego „słownego” znaczenia, to tylko wymiary rzutowania. | PC1, PC2 mają jasną interpretację: PC1 ≈ „poziom rozwoju”, PC2 ≈ „przemysł vs usługi”.    |
| **Miara dopasowania** | STRESS (u nas Sammon 2D ≈ 0.04 – bardzo dobre dopasowanie).     | Udział wariancji, wartości własne (PC1 + PC2 ≈ 74% całej zmienności w danych).           |
| **Zastosowanie**   | Bardzo dobra „mapa podobieństw” krajów – kto koło kogo leży.      | Dobra baza do interpretacji – biplot, wpływ zmiennych, opis osi w zwykłym języku.        |


**Podsumowanie:**

- MDS i PCA ustawiają kraje prawie tak samo (wysokie korelacje: PC1 z MDS1, PC2 z MDS2 – różni się głównie znak osi).
- **MDS (Sammon)** jest idealny do „obrazka”: widać, które kraje są sobie bliskie, a które bardzo odległe.
- **PCA** daje podobne ułożenie krajów, ale dodatkowo mówi *dlaczego* kraje są w takich miejscach –  
  główna oś PC1 to bogactwo/usługi vs bieda/rolnictwo, a PC2 to bardziej przemysł vs usługi.
  
  
  
## 7. Podsumowanie i wnioski końcowe

### 7.1. Co najbardziej różnicuje kraje?

- Najmocniej różnią: **PKB na mieszkańca, śmiertelność niemowląt, alfabetyzm, liczba telefonów, udział rolnictwa i usług**.  
- W PCA:
  - **PC1 (~58%)** ≈ oś „**bogate/usługowe vs biedne/rolnicze**”.
  - **PC2 (~16%)** ≈ „**przemysł vs usługi**”.
- **2 składowe (PC1–PC2)** dają już sensowny opis danych (ok. 74% wariancji).

### 7.2. Jakie grupy krajów widać?

- **Bogate, usługowe** (Europa Zach., Ameryka Płn., część Oceanii) – wysokie PC1, blisko siebie na MDS i PCA.  
- **Biedniejsze, rolnicze** (głównie Afryka Subsaharyjska) – niskie PC1, druga strona wykresów.  
- **Kraje „pośrednie”** (część Ameryki Łacińskiej, Europy Wsch.) – w środku obu map.

### 7.3. MDS vs PCA – praktyczna przydatność

- **MDS (Sammon)**  
  - pokazuje **kto jest blisko kogo** (mapa podobieństw),  
  - bardzo dobry STRESS 2D ≈ 0.04,  
  - **osie nie mają prostego znaczenia** – ważne są tylko odległości.

- **PCA**  
  - pokazuje **to samo ułożenie krajów**,  
  - dodatkowo mówi **DLACZEGO**:  
    - PC1 – poziom rozwoju (PKB, usługi, śmiertelność itd.),  
    - PC2 – przemysł vs usługi,  
  - dobre do interpretacji i biplotu.

### 7.4. Ograniczenia i co dalej

- Dane są **uśrednione w czasie** (brak dynamiki),  
- Analiza jest **przekrojowa**, tylko wybrane wskaźniki, 194 kraje bez braków.  
- Dalsze możliwe kroki:
  - **klasteryzacja** krajów na bazie PC1–PC2 lub MDS,  
  - analiza osobno w regionach,  
  - dodanie nowych zmiennych (np. CO₂, HDI), porównanie wyników.



